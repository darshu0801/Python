import pandas as pd
from sklearn.ensemble import IsolationForest

# Load power data
# Replace 'power_data.csv' with your actual file path
data = pd.read_csv('power_data.csv')  # Ensure the file has 'timestamp' and 'power' columns

def remove_variable_duration_spikes(df):
    """
    Use Isolation Forest to detect and remove spikes in power data with variable durations.
    """
    # Initialize Isolation Forest
    iso_forest = IsolationForest(contamination=0.01, random_state=42)
    
    # Fit the model on the power column
    df['anomaly'] = iso_forest.fit_predict(df[['power']])
    
    # Keep only normal data (-1 indicates anomalies)
    cleaned_df = df[df['anomaly'] != -1].drop(columns=['anomaly'])
    return cleaned_df

# Apply the spike removal
cleaned_data = remove_variable_duration_spikes(data)

# Save the cleaned data to a new CSV file
cleaned_data.to_csv('cleaned_power_data.csv', index=False)

print("Spikes with variable duration removed. Cleaned data saved to 'cleaned_power_data.csv'.")









import pandas as pd
from sklearn.ensemble import IsolationForest

# Load the Excel file
# Replace 'power_data.xlsx' with your actual file path
file_path = 'power_data.xlsx'
sheet_name = 'data'
column_name = 'P_avg'

def remove_variable_duration_spikes(df, column):
    """
    Use Isolation Forest to detect and remove spikes in power data with variable durations.
    """
    # Initialize Isolation Forest
    iso_forest = IsolationForest(contamination=0.01, random_state=42)
    
    # Fit the model on the specified column
    df['anomaly'] = iso_forest.fit_predict(df[[column]])
    
    # Keep only normal data (-1 indicates anomalies)
    cleaned_df = df[df['anomaly'] != -1].drop(columns=['anomaly'])
    return cleaned_df

# Read the specific sheet and column
data = pd.read_excel(file_path, sheet_name=sheet_name)
if column_name not in data.columns:
    raise ValueError(f"Column '{column_name}' not found in sheet '{sheet_name}'.")

# Apply the spike removal
cleaned_data = remove_variable_duration_spikes(data, column_name)

# Save the cleaned data back to an Excel file
output_file = 'cleaned_power_data.xlsx'
cleaned_data.to_excel(output_file, index=False)

print(f"Spikes removed. Cleaned data saved to '{output_file}'.")










import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest

# Load the Excel file
file_path = 'power_data.xlsx'
sheet_name = 'data'
column_name = 'P_avg'

def preprocess_data(df, column):
    """
    Apply smoothing to preprocess data for better anomaly detection.
    """
    # Rolling median to reduce noise
    df[f'{column}_smoothed'] = df[column].rolling(window=5, center=True).median()
    # Fill NaN values introduced by rolling
    df[f'{column}_smoothed'].fillna(df[column], inplace=True)
    return df

def remove_spikes_with_isolation_forest(df, column):
    """
    Use Isolation Forest to detect and remove spikes.
    """
    # Initialize Isolation Forest
    iso_forest = IsolationForest(contamination=0.01, random_state=42)
    
    # Fit the model on smoothed power values
    df['anomaly'] = iso_forest.fit_predict(df[[column]])
    
    # Keep only normal data (-1 indicates anomalies)
    cleaned_df = df[df['anomaly'] != -1].drop(columns=['anomaly'])
    return cleaned_df

def remove_residual_spikes(df, column):
    """
    Remove residual spikes using Z-score analysis.
    """
    # Calculate Z-scores
    df['z_score'] = (df[column] - df[column].mean()) / df[column].std()
    # Keep rows where the Z-score is within the normal range
    cleaned_df = df[np.abs(df['z_score']) < 3].drop(columns=['z_score'])
    return cleaned_df

# Step 1: Read the Excel file
data = pd.read_excel(file_path, sheet_name=sheet_name)
if column_name not in data.columns:
    raise ValueError(f"Column '{column_name}' not found in sheet '{sheet_name}'.")

# Step 2: Preprocess data (smoothing)
data = preprocess_data(data, column_name)

# Step 3: Remove spikes using Isolation Forest
cleaned_data = remove_spikes_with_isolation_forest(data, f'{column_name}_smoothed')

# Step 4: Remove residual spikes using Z-score analysis
final_cleaned_data = remove_residual_spikes(cleaned_data, f'{column_name}_smoothed')

# Step 5: Save the cleaned data back to an Excel file
output_file = 'cleaned_power_data.xlsx'
final_cleaned_data.to_excel(output_file, index=False)

print(f"Spikes removed and cleaned data saved to '{output_file}'.")









import pandas as pd

# Load the Excel file
file_path = 'power_data.xlsx'
sheet_name = 'data'
column_name = 'P_avg'

def detect_and_remove_spikes(df, column, spike_duration=15):
    """
    Detect spikes based on sudden changes and duration, and replace them with 0.
    """
    # Create a column to store spike-removed values
    df['cleaned_power'] = df[column]

    # Identify sudden changes
    sudden_change = (df[column].diff().abs() > 0.5)  # Change threshold (adjust 0.5 as needed)
    df['is_spike'] = 0
    df.loc[sudden_change, 'is_spike'] = 1

    # Track spike duration
    spike_start = None
    for i in range(len(df)):
        if df.loc[i, 'is_spike'] == 1:
            if spike_start is None:
                spike_start = i
        else:
            if spike_start is not None:
                duration = (df.loc[i, 'timestamp'] - df.loc[spike_start, 'timestamp']).total_seconds()
                if duration < spike_duration:
                    # Mark the spike period as invalid (set values to 0)
                    df.loc[spike_start:i, 'cleaned_power'] = 0
                spike_start = None

    return df

# Read the specific sheet and column
data = pd.read_excel(file_path, sheet_name=sheet_name)

# Ensure the timestamp column exists
if 'timestamp' not in data.columns or column_name not in data.columns:
    raise ValueError(f"Ensure 'timestamp' and '{column_name}' columns are present in the sheet '{sheet_name}'.")

# Convert timestamp to datetime for duration calculations
data['timestamp'] = pd.to_datetime(data['timestamp'])

# Detect and remove spikes
cleaned_data = detect_and_remove_spikes(data, column_name)

# Save the cleaned data to a new Excel file
output_file = 'cleaned_power_data.xlsx'
cleaned_data.to_excel(output_file, index=False)

print(f"Spikes removed and cleaned data saved to '{output_file}'.")